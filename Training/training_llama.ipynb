{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOeUhJfvwQ+8epEoMwZCUOS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  }
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apetulante/Tutorials/blob/master/Training/training_llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning LLaMa 3.2\n",
        "\n",
        "This notebook demonstrates how to fine-tune the LLaMa 3.2 (1B model), Meta's latest small-scale LLM. We use the 1B parameter version to ensure this can run effectively on Google Colab.\n",
        "\n",
        "Key features of LLaMa 3.2 1B:\n",
        "- 1.23B parameters\n",
        "- Multilingual support (8 officially supported languages)\n",
        "- 128k context length\n",
        "- Optimized for dialogue use cases\n",
        "\n",
        "As we'll see, this same notebook can be used to train other, larger versions of LlaMa 3.2, simply by swapping the HuggingFace repo to one of the larger versions.\n",
        "\n"
      ],
      "metadata": {
        "id": "gAsvqrahL8WM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup\n"
      ],
      "metadata": {
        "id": "z-ft6hpQK_PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package Installation + Imports\n",
        "\n",
        "First, let's install the required packages. We'll need the latest version of transformers (>= 4.43.0) to work with LLaMa 3.2."
      ],
      "metadata": {
        "id": "hDYxyiRTLIRu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYjfbQJZH29C",
        "outputId": "5edba780-9c9e-4282-a6da-aca7d61c4a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m337.9/480.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q --upgrade transformers datasets\n",
        "!pip install -q torch accelerate bitsandbytes\n",
        "# Install PDF processing libraries. pdfplumber handles academic texts better\n",
        "!pip install -q PyPDF2\n",
        "!pip install -q pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard libraries\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "# AI/ML Libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import pipeline\n",
        "from accelerate.state import AcceleratorState\n",
        "from accelerate import Accelerator\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import wandb # this will be optional, for training monitoring\n",
        "\n",
        "# PDF reading\n",
        "import pdfplumber\n",
        "from pdfminer.high_level import extract_text\n",
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "q_yvuVAUxucp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Package Overview\n",
        "- `transformers`: Hugging Face's main library for working with transformer models\n",
        "- `torch`: PyTorch deep learning framework\n",
        "- `accelerate`: Library for easy mixed precision training and device placement\n",
        "- `pdfminer`: A PDF-reading package that's particularly good for non-straightforward PDF's (like academic papers)\n"
      ],
      "metadata": {
        "id": "HiOJjfe7RLlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Check\n",
        "Let's verify our setup and check available compute resources:\n",
        "\n",
        "Make sure that `CUDA available`: prints **True**"
      ],
      "metadata": {
        "id": "3mjl7aNgLOF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check PyTorch version and CUDA availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKnWLKfGRWRL",
        "outputId": "85c6fffd-ce75-42eb-df16-50cad9ca9099"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA A100-SXM4-40GB\n",
            "Available GPU memory: 39.56 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face Authentication\n",
        "\n",
        "LLaMa 3.2 requires authentication with Hugging Face to access the model. You'll need to:\n",
        "1. Have a Hugging Face account\n",
        "2. Accept the LLaMa 3.2 model terms of use on the Hugging Face model page\n",
        "3. Create an access token on Hugging Face (https://huggingface.co/settings/tokens)\n",
        "\n",
        "After you have your access token and have accepted the terms, the code below will help you log in:"
      ],
      "metadata": {
        "id": "BSe7ErY8Ur6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import getpass\n",
        "\n",
        "token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "login(token=token)\n",
        "\n",
        "# Verify login\n",
        "print(\"Login status: Authenticated with Hugging Face\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNKjESjjUqR-",
        "outputId": "891bf7a3-5b2f-472e-d142-467d305d4ab7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token: ··········\n",
            "Login status: Authenticated with Hugging Face\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer Setup and Exploration\n",
        "\n",
        "LLaMa 3.2 uses a sophisticated tokenizer that supports multiple languages. Understanding how the tokenizer works is crucial for:\n",
        "- Preparing training data effectively\n",
        "- Managing sequence lengths\n",
        "- Understanding model behavior across languages\n",
        "\n",
        "Let's load the tokenizer and explore its basic properties:"
      ],
      "metadata": {
        "id": "qZEfRS18RVf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Make sure padding token is set\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Basic tokenizer information\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
        "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
        "print(f\"Padding token: {tokenizer.pad_token}\")\n",
        "print(f\"End of sequence token: {tokenizer.eos_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287,
          "referenced_widgets": [
            "cf15788654234d4baedb6375aa5099bf",
            "fda679a074d341bdb9852f85732f337c",
            "ee3d902188ca4ee88fab5ff84d44fc7a",
            "f67750dfb4174539ae61cf79a5b6b44f",
            "8095de73fbcf414f8bea00b754a7f012",
            "b04b4c87fcba4ff3b2ce207780f59974",
            "942939210dd74ad4b9bf4db00b4178ea",
            "b1ad8e2a7cd7437282d404150283249d",
            "856571debb5649df9085aad68c822787",
            "3a468ac7420747fcb6b9c44d46975e6b",
            "14953b2e2c084be2a816b156c9520aa8",
            "c80c9b5220ec48749f7e055e03e9abec",
            "701d8e84951643129feefc85247e988f",
            "b21af594573c41ceb5e237a7c763e368",
            "12cbf21d1d0c411185329c81c3102310",
            "11fa96649eaa41aa94c36a665c1df0f1",
            "91848f89c8bd48cbaa0ce30830cfac83",
            "a98a2f8ad73c458c9a1400625dd30cef",
            "76958a274d614162913faa06aab109c5",
            "2d943fc31a424186aaade3d2aa5c55ef",
            "536a76b59ecb481584bc128417113045",
            "867cbd556cfa499c9794882d6acc5542",
            "3c15f1a897ce48109252faee8948604a",
            "62742b1f8cee4f6aa9444d2a6a16c613",
            "7133b632460c45f9865bc81c0f57f3f3",
            "396ae717a3604951b9aa9ed0284cbfa1",
            "07e128dbfbc44dca8034abfbee22ab1b",
            "d12d89419a104c5d83c0f041a350d101",
            "f0a739330ac64a71ad30cf2c276a6282",
            "5002542b518a42feb32c852b639937f8",
            "87993bd8674d43cdbea8592d6d08e5a8",
            "c141ce46a8f441329ed391f1abadec09",
            "5c2bb8722c5d4122beedf5fd168a626f"
          ]
        },
        "id": "5S_rxI9aUMu1",
        "outputId": "f92b6d67-2520-401c-aa4e-4e3a73767cc0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf15788654234d4baedb6375aa5099bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c80c9b5220ec48749f7e055e03e9abec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c15f1a897ce48109252faee8948604a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 128256\n",
            "Model max length: 131072\n",
            "Padding token: <|end_of_text|>\n",
            "End of sequence token: <|end_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Tokenization\n",
        "\n",
        "Let's see how the tokenizer processes text in different languages. This will help us understand:\n",
        "- How words are broken into tokens\n",
        "- How special characters are handled\n",
        "- Token counts for different languages"
      ],
      "metadata": {
        "id": "VBJpzuBEUYXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example texts in different languages\n",
        "texts = {\n",
        "    \"English\": \"Hello, how are you today?\",\n",
        "    \"Spanish\": \"¡Hola! ¿Cómo estás hoy?\",\n",
        "    \"French\": \"Bonjour! Comment allez-vous aujourd'hui?\",\n",
        "    \"German\": \"Hallo! Wie geht es dir heute?\"\n",
        "}\n",
        "\n",
        "# Analyze tokenization for each language\n",
        "for lang, text in texts.items():\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_ids = tokenizer.encode(text)\n",
        "\n",
        "    print(f\"\\n{lang}:\")\n",
        "    print(f\"Original text: {text}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Number of tokens: {len(tokens)}\")\n",
        "    print(f\"Token IDs: {token_ids}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEYn8lFlhJNu",
        "outputId": "7146f39b-4e42-491e-e254-dfb9ce4cff1d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English:\n",
            "Original text: Hello, how are you today?\n",
            "Tokens: ['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', 'Ġtoday', '?']\n",
            "Number of tokens: 7\n",
            "Token IDs: [128000, 9906, 11, 1268, 527, 499, 3432, 30]\n",
            "\n",
            "Spanish:\n",
            "Original text: ¡Hola! ¿Cómo estás hoy?\n",
            "Tokens: ['Â¡', 'Hola', '!', 'ĠÂ¿', 'CÃ³mo', 'Ġest', 'Ã¡s', 'Ġhoy', '?']\n",
            "Number of tokens: 9\n",
            "Token IDs: [128000, 40932, 69112, 0, 29386, 96997, 1826, 7206, 49841, 30]\n",
            "\n",
            "French:\n",
            "Original text: Bonjour! Comment allez-vous aujourd'hui?\n",
            "Tokens: ['Bonjour', '!', 'ĠComment', 'Ġalle', 'z', '-vous', 'Ġaujourd', \"'hui\", '?']\n",
            "Number of tokens: 9\n",
            "Token IDs: [128000, 82681, 0, 12535, 12584, 89, 45325, 75804, 88253, 30]\n",
            "\n",
            "German:\n",
            "Original text: Hallo! Wie geht es dir heute?\n",
            "Tokens: ['Hallo', '!', 'ĠWie', 'Ġgeht', 'Ġes', 'Ġdir', 'Ġheute', '?']\n",
            "Number of tokens: 8\n",
            "Token IDs: [128000, 79178, 0, 43716, 40364, 1560, 5534, 49714, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Notice how:\n",
        "1. Punctuations are their own tokens\n",
        "2. The 'Ġ' symbol represents a space before the token\n",
        "3. Some words are single tokens (like 'today') while others might be split"
      ],
      "metadata": {
        "id": "iLfE92eUwMOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation for Fine-tuning\n",
        "\n",
        "This notebook demonstrates how to update LLaMa 3.2's knowledge base with domain-specific information. This type of fine-tuning can help the model:\n",
        "- Learn new domain-specific facts and concepts\n",
        "- Update its knowledge about specific topics\n",
        "- Improve its ability to discuss specialized subjects\n",
        "\n",
        "For domain knowledge fine-tuning, we'll use the simplest appraoch of direct text chunks: the model will learn directly from the source material."
      ],
      "metadata": {
        "id": "84ivn4uql6rM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structuring the Data\n",
        "\n",
        "Fine-tuning LLaMa 3.2 requires carefully formatted training data. The model expects:\n",
        "- Input text in a specific format\n",
        "- Response text that follows the input\n",
        "- Proper formatting of system prompts and chat turns\n",
        "\n",
        "We'll set up our dataset as such in this section."
      ],
      "metadata": {
        "id": "xA0Tm_5oSm9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distinct pieces of data for the model are relatively small text segments. So here, we'll start by setting up a function to create chunks of text from longer, full documents for the model to train on.\n",
        "\n",
        "### Important Parameters:\n",
        "- `chunk_size`: Default 512 tokens. Can be adjusted based on your GPU memory and needs\n",
        "- `tokenizer`: Uses LLaMa's tokenizer to ensure proper text splitting\n"
      ],
      "metadata": {
        "id": "0FftSeZbmxeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_text_chunks(document: str, tokenizer, chunk_size: int = 512) -> List[str]:\n",
        "    \"\"\"\n",
        "    Create chunks of text that:\n",
        "    - Maintain sentence boundaries where possible\n",
        "    - Have a reasonable minimum size\n",
        "    - Don't include special tokens\n",
        "    \"\"\"\n",
        "    # Clean the text first\n",
        "    document = document.strip().replace('\\n', ' ')\n",
        "\n",
        "    # Split into sentences\n",
        "    sentences = [s.strip() + '.' for s in document.split('.') if s.strip()]\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Get token count for this sentence\n",
        "        tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
        "        sentence_length = len(tokens)\n",
        "\n",
        "        if current_length + sentence_length > chunk_size:\n",
        "            if current_chunk:\n",
        "                # Join the current chunk and add it to chunks\n",
        "                chunk_text = ' '.join(current_chunk)\n",
        "                chunks.append(chunk_text)\n",
        "                # Start new chunk with current sentence\n",
        "                current_chunk = [sentence]\n",
        "                current_length = sentence_length\n",
        "        else:\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += sentence_length\n",
        "\n",
        "    # Don't forget the last chunk\n",
        "    if current_chunk:\n",
        "        chunk_text = ' '.join(current_chunk)\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    # Print some diagnostics\n",
        "    total_tokens = sum(len(tokenizer.encode(chunk, add_special_tokens=False))\n",
        "                      for chunk in chunks)\n",
        "\n",
        "    print(f\"Created {len(chunks)} chunks with total {total_tokens} tokens\")\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "MUVdp5zomw4W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use an example chunk of text to see how this works before proceeding."
      ],
      "metadata": {
        "id": "-ommRhmUz0xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example document - we'll replace this with actual domain content later\n",
        "sample_document = \"\"\"\n",
        "LLMs process text using attention mechanisms. These mechanisms allow the model\n",
        "to weigh different parts of the input differently. The transformer architecture\n",
        "revolutionized natural language processing. It introduced self-attention as a\n",
        "core component. Modern language models build upon this foundation. As such, LLM's\n",
        "are able to process sentences similar to the way that humans do, by understanding\n",
        "words in the sentence relative to those around them.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"\\nTesting with chunk_size=20:\")\n",
        "chunks = create_text_chunks(sample_document, tokenizer, chunk_size=20)\n",
        "for i, chunk in enumerate(chunks[0:2]):\n",
        "    tokens = tokenizer.tokenize(chunk)\n",
        "    print(f\"\\nChunk {i+1}:\")\n",
        "    print(f\"Text: {chunk}\")\n",
        "    print(f\"Token count: {len(tokens)}\")\n",
        "    print(f\"Tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEgHXy90unMV",
        "outputId": "a4b667cd-0ac7-4200-975a-381b2fdabaa4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing with chunk_size=20:\n",
            "Created 5 chunks with total 80 tokens\n",
            "\n",
            "Chunk 1:\n",
            "Text: LLMs process text using attention mechanisms.\n",
            "Token count: 8\n",
            "Tokens: ['LL', 'Ms', 'Ġprocess', 'Ġtext', 'Ġusing', 'Ġattention', 'Ġmechanisms', '.']\n",
            "\n",
            "Chunk 2:\n",
            "Text: These mechanisms allow the model to weigh different parts of the input differently.\n",
            "Token count: 14\n",
            "Tokens: ['These', 'Ġmechanisms', 'Ġallow', 'Ġthe', 'Ġmodel', 'Ġto', 'Ġweigh', 'Ġdifferent', 'Ġparts', 'Ġof', 'Ġthe', 'Ġinput', 'Ġdifferently', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Processing PDF Documents\n",
        "\n",
        "We'll be creating these chunks from text documents.\n",
        "\n",
        "In reality, we'll want to accommodate giving some PDF's as a document set to fine-tune on. We'll use pdfminer to read PDFs and extract their text content. Then we'll process this text into appropriate chunks for training."
      ],
      "metadata": {
        "id": "GcyMjC7pxBpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdfs(pdf_paths: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract text from multiple PDF files.\n",
        "\n",
        "    Args:\n",
        "        pdf_paths: List of paths to PDF files\n",
        "\n",
        "    Returns:\n",
        "        List of extracted text documents\n",
        "    \"\"\"\n",
        "    all_texts = []\n",
        "\n",
        "    for path in pdf_paths:\n",
        "        try:\n",
        "            text = extract_text(path)\n",
        "            all_texts.append(text)\n",
        "            print(f\"Successfully processed: {path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {path}: {str(e)}\")\n",
        "\n",
        "    return all_texts"
      ],
      "metadata": {
        "id": "KJB6_EvXmCdl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Academic papers often contain a lot of irregular, non-text components. Between images, captions, references, etc, there's a lot that can confuse the model if we aren't careful to remove it. So here, we'll write a function that cleans our text after we read it, attempting to remove as many of these artifacts as possible."
      ],
      "metadata": {
        "id": "_sMc-MNZ9Csa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_academic_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean academic text with improved word separation.\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    original_length = len(text)\n",
        "\n",
        "    # First clean pass - basic normalization\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "\n",
        "    # Fix incorrectly joined words\n",
        "    # Look for patterns like \"wordWord\" or \"word-Word\"\n",
        "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  # Split \"wordWord\"\n",
        "    text = re.sub(r'([a-z])-\\s*([a-z])', r'\\1\\2', text)  # Join \"wo rd\"\n",
        "    text = re.sub(r'\\s*-\\s*', '-', text)  # Clean up hyphens\n",
        "\n",
        "    # Fix common academic text patterns\n",
        "    patterns = {\n",
        "        r'(?<=\\w)\\.(?=\\w)': '. ',  # Add space after period between words\n",
        "        r'(?<=\\w)\\s+\\(': ' (',      # Fix spacing around parentheses\n",
        "        r'\\)\\s+(?=\\w)': ') ',\n",
        "        r'(?<=\\d),(?=\\d)': ', ',    # Add space after comma between numbers\n",
        "        r'(?<=[a-z])(?=\\d)': ' ',   # Add space between letters and numbers\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in patterns.items():\n",
        "        text = re.sub(pattern, replacement, text)\n",
        "\n",
        "    # Remove common PDF artifacts\n",
        "    artifacts = [\n",
        "        r'Fig\\.\\s*\\d+',\n",
        "        r'Figure\\s*\\d+:?.*?\\n',\n",
        "        r'Table\\s*\\d+:?.*?\\n',\n",
        "        r'\\[\\d+(?:,\\s*\\d+)*\\]',     # Citations [1] or [1,2,3]\n",
        "        r'\\(\\w+\\s+et\\s+al\\.,\\s+\\d{4}\\)',  # Citations (Author et al., 2020)\n",
        "        r'References.*$',            # Remove references section\n",
        "        r'Bibliography.*$',\n",
        "    ]\n",
        "\n",
        "    for pattern in artifacts:\n",
        "        text = re.sub(pattern, ' ', text)\n",
        "\n",
        "    # Final cleanup\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces again\n",
        "    text = text.strip()\n",
        "\n",
        "    # Print sample before/after for verification\n",
        "    print(\"\\nSample text cleaning comparison:\")\n",
        "    print(\"Original first 100 chars:\", text[:100])\n",
        "    sample_cleaned = text[:100]\n",
        "    print(\"Cleaned first 100 chars:\", sample_cleaned)\n",
        "\n",
        "    final_length = len(text)\n",
        "    removed = original_length - final_length\n",
        "    if removed > 0:\n",
        "        print(f\"\\nRemoved {removed} characters ({removed/original_length*100:.1f}% of original text)\")\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "vN77Yo509UIl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formatting Text Chunks for LLaMa\n",
        "\n",
        "Finally, we need to properly format our text chunks into a format that the model expects and understands.\n",
        "\n",
        "Each chunk will be formatted as:\n",
        "\n",
        "`<|system|>Learn the following information: </s><|user|>{cleaned_text}</s><|assistant|>I understand this information.</s>`\n",
        "\n",
        "This format follows LLaMa's chat template structure, which is crucial because:\n",
        "\n",
        "1. LLaMa 3.2 was trained using a specific chat format with different roles:\n",
        "   - `<|system|>`: Provides context or instructions to the model\n",
        "   - `<|user|>`: Represents input content\n",
        "   - `</s>`: Special token marking the end of each turn\n",
        "\n",
        "2. The model expects input in the same format it was originally trained on, so using a different format might confuse the model or reduce learning effectiveness"
      ],
      "metadata": {
        "id": "VxRsJvm5TYED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_training_example(text: str) -> str:\n",
        "    \"\"\"Format text for training\"\"\"\n",
        "    cleaned_text = text.strip()\n",
        "    if not cleaned_text:  # Skip empty chunks\n",
        "        return None\n",
        "    return f\"<|system|>Learn the following information: </s><|user|>{cleaned_text}</s><|assistant|>I understand this information.</s>\""
      ],
      "metadata": {
        "id": "iRr0E2oDTbJA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF's to Training Data Pipeline\n",
        "\n",
        "We'll use these above functions to now build a pipeline for converting PDF documents into a format suitable for training LLaMa 3.2. The process involves:\n",
        "1. Loading PDFs and extracting text\n",
        "2. Cleaning and preprocessing the text\n",
        "3. Chunking the text into appropriate sizes\n",
        "4. Creating a dataset for training"
      ],
      "metadata": {
        "id": "zDcmjOwl1aea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def create_training_dataset(pdf_directory, tokenizer, chunk_size=512):\n",
        "    \"\"\"Create a training dataset from PDF documents\"\"\"\n",
        "\n",
        "    # First get all PDFs and their text\n",
        "    pdf_paths = [f\"{pdf_directory}/{f}\" for f in os.listdir(pdf_directory)\n",
        "                if f.endswith('.pdf')]\n",
        "\n",
        "    print(f\"Found {len(pdf_paths)} PDFs\")\n",
        "\n",
        "    # Extract text from PDFs\n",
        "    documents = extract_text_from_pdfs(pdf_paths)\n",
        "    print(f\"Extracted text from {len(documents)} documents\")\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    for doc in documents:\n",
        "        # Clean text first\n",
        "        text = clean_academic_text(doc)\n",
        "        # Create chunks from the document\n",
        "        chunks = create_text_chunks(text, tokenizer, chunk_size)\n",
        "\n",
        "        # Format each chunk\n",
        "        formatted_chunks = [format_training_example(chunk) for chunk in chunks]\n",
        "        all_chunks.extend(formatted_chunks)\n",
        "    print(\"\\n\")\n",
        "    print(f\"\\033[1m\\033[91mCreated {len(all_chunks)} total chunks\\033[0m\")\n",
        "\n",
        "    # Create dataset dictionary\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"labels\": []\n",
        "    }\n",
        "\n",
        "    # Process each chunk\n",
        "    for chunk in all_chunks:\n",
        "        encodings = tokenizer(\n",
        "            chunk,\n",
        "            truncation=True,\n",
        "            max_length=chunk_size,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        dataset_dict[\"input_ids\"].append(encodings[\"input_ids\"].squeeze().numpy())  # Convert to numpy\n",
        "        dataset_dict[\"attention_mask\"].append(encodings[\"attention_mask\"].squeeze().numpy())\n",
        "        dataset_dict[\"labels\"].append(encodings[\"input_ids\"].squeeze().numpy())\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    for key in dataset_dict:\n",
        "        if dataset_dict[key]:  # Check if the list is not empty\n",
        "            dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    return Dataset.from_dict(dataset_dict)"
      ],
      "metadata": {
        "id": "6gBRGY_O2__7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading our Docs + Making the Dataset\n",
        "\n",
        "Now, all that's left is to create a directory of pdf's that we can point the dataset generation code to.\n",
        "\n",
        "Below, you can upload a set of PDF's that will become the training data.\n",
        "\n",
        "This will be automatically saved to a path called training_pdfs, which we'll use for setting up our dataset in the following cells."
      ],
      "metadata": {
        "id": "g7vW7m-9031m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Create a directory for PDFs\n",
        "!mkdir -p training_pdfs\n",
        "\n",
        "print(\"Please upload your PDF files using the file uploader that appears below:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to our PDF directory\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, f\"training_pdfs/{filename}\")\n",
        "    print(f\"Moved {filename} to training_pdfs/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "id": "Z45yxu0h0CSH",
        "outputId": "73b152f3-9b44-4331-a10f-96f1058a7f9d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your PDF files using the file uploader that appears below:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a7517f2-63f9-49d1-8e2b-566172044873\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a7517f2-63f9-49d1-8e2b-566172044873\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1-s2.0-S0193397316300569-main.pdf to 1-s2.0-S0193397316300569-main.pdf\n",
            "Saving 10283741.pdf to 10283741.pdf\n",
            "Saving A_picture_book_reading_interve.pdf to A_picture_book_reading_interve.pdf\n",
            "Saving children-09-01149.pdf to children-09-01149.pdf\n",
            "Saving crain-thoreson-dale-1999-enhancing-linguistic-performance-parents-and-teachers-as-book-reading-partners-for-children.pdf to crain-thoreson-dale-1999-enhancing-linguistic-performance-parents-and-teachers-as-book-reading-partners-for-children.pdf\n",
            "Saving DR Info - ccf.fiu.edu.docx to DR Info - ccf.fiu.edu.docx\n",
            "Saving DR Info - extensionpubs.unl.edu.docx to DR Info - extensionpubs.unl.edu.docx\n",
            "Saving DR Info - flvpkonline.org.docx to DR Info - flvpkonline.org.docx\n",
            "Saving DR Info - ReadingRockets.org.docx to DR Info - ReadingRockets.org.docx\n",
            "Saving DR Info - Sol Book Box.docx to DR Info - Sol Book Box.docx\n",
            "Saving DR Info - Twinkl.com.docx to DR Info - Twinkl.com.docx\n",
            "Saving DR Info - VoyagerSopris.com.docx to DR Info - VoyagerSopris.com.docx\n",
            "Saving flynn-2011-developing-children-s-oral-language-skills-through-dialogic-reading-guidelines-for-implementation.pdf to flynn-2011-developing-children-s-oral-language-skills-through-dialogic-reading-guidelines-for-implementation.pdf\n",
            "Saving homedrguide_english.pdf to homedrguide_english.pdf\n",
            "Saving Instructing Parents to Use Dialogic Reading Strategies with Preschool Children.pdf to Instructing Parents to Use Dialogic Reading Strategies with Preschool Children.pdf\n",
            "Saving margaret_dissertation_intro.docx to margaret_dissertation_intro.docx\n",
            "Saving nihms-678472.pdf to nihms-678472.pdf\n",
            "Moved 1-s2.0-S0193397316300569-main.pdf to training_pdfs/\n",
            "Moved 10283741.pdf to training_pdfs/\n",
            "Moved A_picture_book_reading_interve.pdf to training_pdfs/\n",
            "Moved children-09-01149.pdf to training_pdfs/\n",
            "Moved crain-thoreson-dale-1999-enhancing-linguistic-performance-parents-and-teachers-as-book-reading-partners-for-children.pdf to training_pdfs/\n",
            "Moved DR Info - ccf.fiu.edu.docx to training_pdfs/\n",
            "Moved DR Info - extensionpubs.unl.edu.docx to training_pdfs/\n",
            "Moved DR Info - flvpkonline.org.docx to training_pdfs/\n",
            "Moved DR Info - ReadingRockets.org.docx to training_pdfs/\n",
            "Moved DR Info - Sol Book Box.docx to training_pdfs/\n",
            "Moved DR Info - Twinkl.com.docx to training_pdfs/\n",
            "Moved DR Info - VoyagerSopris.com.docx to training_pdfs/\n",
            "Moved flynn-2011-developing-children-s-oral-language-skills-through-dialogic-reading-guidelines-for-implementation.pdf to training_pdfs/\n",
            "Moved homedrguide_english.pdf to training_pdfs/\n",
            "Moved Instructing Parents to Use Dialogic Reading Strategies with Preschool Children.pdf to training_pdfs/\n",
            "Moved margaret_dissertation_intro.docx to training_pdfs/\n",
            "Moved nihms-678472.pdf to training_pdfs/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And putting it togehter:"
      ],
      "metadata": {
        "id": "kZY1zS3EUt9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "training_dataset = create_training_dataset(\"training_pdfs\", tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzu2b3O25vQZ",
        "outputId": "932be03b-430c-4a20-e686-fc6596579bee"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 PDFs\n",
            "Successfully processed: training_pdfs/flynn-2011-developing-children-s-oral-language-skills-through-dialogic-reading-guidelines-for-implementation.pdf\n",
            "Successfully processed: training_pdfs/homedrguide_english.pdf\n",
            "Successfully processed: training_pdfs/1-s2.0-S0193397316300569-main.pdf\n",
            "Successfully processed: training_pdfs/Instructing Parents to Use Dialogic Reading Strategies with Preschool Children.pdf\n",
            "Successfully processed: training_pdfs/children-09-01149.pdf\n",
            "Successfully processed: training_pdfs/10283741.pdf\n",
            "Successfully processed: training_pdfs/crain-thoreson-dale-1999-enhancing-linguistic-performance-parents-and-teachers-as-book-reading-partners-for-children.pdf\n",
            "Successfully processed: training_pdfs/A_picture_book_reading_interve.pdf\n",
            "Successfully processed: training_pdfs/nihms-678472.pdf\n",
            "Extracted text from 9 documents\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: Academics Developing Children’s Oral Language Skills Through Dialogic Reading Guidelines for Impleme\n",
            "Cleaned first 100 chars: Academics Developing Children’s Oral Language Skills Through Dialogic Reading Guidelines for Impleme\n",
            "\n",
            "Removed 9919 characters (29.7% of original text)\n",
            "Created 11 chunks with total 5108 tokens\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: Reading Explorers Program C A R E G I V E R G U I D E F O R D I A L O G I C R E A D I N G What is Di\n",
            "Cleaned first 100 chars: Reading Explorers Program C A R E G I V E R G U I D E F O R D I A L O G I C R E A D I N G What is Di\n",
            "\n",
            "Removed 70 characters (1.1% of original text)\n",
            "Created 3 chunks with total 1327 tokens\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: Journal of Applied Developmental Psychology 46 (2016) 51–62 Contents lists available at Science Dire\n",
            "Cleaned first 100 chars: Journal of Applied Developmental Psychology 46 (2016) 51–62 Contents lists available at Science Dire\n",
            "\n",
            "Removed 24500 characters (24.8% of original text)\n",
            "Created 37 chunks with total 17716 tokens\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: Journal of Applied School Psychology ISSN: 1537-7903 (Print) 1537-7911 (Online) Journal homepage: ww\n",
            "Cleaned first 100 chars: Journal of Applied School Psychology ISSN: 1537-7903 (Print) 1537-7911 (Online) Journal homepage: ww\n",
            "\n",
            "Removed 406 characters (1.1% of original text)\n",
            "Created 16 chunks with total 7936 tokens\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: Article “Let’s Read Together”: A Parent-Focused Intervention on Dialogic Book Reading to Improve Ear\n",
            "Cleaned first 100 chars: Article “Let’s Read Together”: A Parent-Focused Intervention on Dialogic Book Reading to Improve Ear\n",
            "\n",
            "Removed 8843 characters (11.6% of original text)\n",
            "Created 32 chunks with total 15836 tokens\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: Parent-EMBRACE: An Adaptive Dialogic Reading Intervention Arun Balajiee Lekshmi Narayanan 1[000-0002\n",
            "Cleaned first 100 chars: Parent-EMBRACE: An Adaptive Dialogic Reading Intervention Arun Balajiee Lekshmi Narayanan 1[000-0002\n",
            "\n",
            "Removed 4727 characters (30.2% of original text)\n",
            "Created 5 chunks with total 2226 tokens\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: 28 Enhancing Linguistic Performance: Parents and Teachers as Book Reading Partners for Children with\n",
            "Cleaned first 100 chars: 28 Enhancing Linguistic Performance: Parents and Teachers as Book Reading Partners for Children with\n",
            "\n",
            "Removed 1961 characters (3.7% of original text)\n",
            "Created 22 chunks with total 10841 tokens\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: Developmental Psychology 1994, Vol. 30, No. 5, 679-689 Co Dvri Kht 1994 by the American Psychologica\n",
            "Cleaned first 100 chars: Developmental Psychology 1994, Vol. 30, No. 5, 679-689 Co Dvri Kht 1994 by the American Psychologica\n",
            "\n",
            "Removed 11001 characters (16.2% of original text)\n",
            "Created 25 chunks with total 12175 tokens\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: A u t h o r M a n u s c r i p t A u t h o r M a n u s c r i p t A u t h o r M a n u s c r i p t A u \n",
            "Cleaned first 100 chars: A u t h o r M a n u s c r i p t A u t h o r M a n u s c r i p t A u t h o r M a n u s c r i p t A u \n",
            "\n",
            "Removed 11285 characters (38.5% of original text)\n",
            "Created 9 chunks with total 4005 tokens\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[91mCreated 160 total chunks\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Dataset Size\n",
        "\n",
        "The code above will tell you how many total chunks were generated from the PDF's that you uploaded. When fine-tuning LLaMa for domain knowledge, the number of training chunks is crucial:\n",
        "\n",
        "### Recommended Chunk Numbers:\n",
        "- **Minimum**: 200-300 chunks\n",
        " - Below this, the model may not learn effectively\n",
        " - Risk of overfitting to limited examples\n",
        "\n",
        "- **Target**: 500-1000+ chunks\n",
        " - Provides enough examples for robust learning\n",
        " - Allows for diverse phrasings of similar concepts\n",
        "\n",
        "- **Creating More Chunks**:\n",
        " - Add more domain documents\n",
        " - Use overlapping chunks (e.g., 50-token overlap)\n",
        " - Include related papers/documents\n",
        " - Consider smaller chunk sizes (but not below 256 tokens)"
      ],
      "metadata": {
        "id": "XIyHdLxcUwjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training LLaMa 3.2 on Domain Knowledge\n",
        "\n"
      ],
      "metadata": {
        "id": "BMTehEWYLgXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Configuration\n",
        "\n",
        "For fine-tuning LLaMa 3.2, we need to carefully configure several parameters:\n",
        "- Training precision (using bfloat16 for efficiency)\n",
        "- Memory optimization settings\n",
        "- Model configuration parameters\n",
        "\n",
        "We'll start with a basic configuration that works well on Google Colab, but these parameters can be adjusted based on your specific needs and hardware capabilities."
      ],
      "metadata": {
        "id": "-dWtHxoBUic2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, TrainingArguments\n",
        "\n",
        "# Reload model without 8-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"Model configuration:\")\n",
        "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
        "print(f\"Training device: {model.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhpccGhdUZ2x",
        "outputId": "a1dfa37d-ac5b-4a7d-8881-d66b4eb95384"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model configuration:\n",
            "Number of parameters: 1,235,814,400\n",
            "Training device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup and Configuration\n",
        "\n",
        "We'll configure training with:\n",
        "- A relatively small number of epochs since we're fine-tuning\n",
        "- Gradient accumulation to handle larger effective batch sizes\n",
        "- Learning rate with warmup\n",
        "\n",
        "#### Key Training Parameters:\n",
        "- `learning_rate=3e-4`: Higher than typical fine-tuning to allow new knowledge absorption\n",
        "- `per_device_train_batch_size=4`: Adjust based on your GPU memory\n",
        "- `gradient_accumulation_steps=8`: Effectively creates batch_size of 32\n",
        "- `num_train_epochs=3`: Adjust based on dataset size and convergence"
      ],
      "metadata": {
        "id": "QHL0xBEnLi_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Different Model Sizes:\n",
        "\n",
        "This setup can be adapted for different LLaMa 3.2 sizes (1B, 3B, etc.) and different hardware:\n",
        "\n",
        "To use a larger LLaMa model, simply change the model ID:\n",
        "```\n",
        "# For 1B model (current)\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# For 3B model\n",
        "# model_id = \"meta-llama/Llama-3.2-3B\"\n",
        "```\n",
        "\n",
        "Hardware Considerations:\n",
        "- 1B model: Runs on most GPUs with 16GB+ memory\n",
        "- 3B model: Recommended 24GB+ GPU memory\n",
        "- For smaller GPUs: Reduce batch size and increase gradient accumulation\n",
        "- For larger GPUs: Increase batch size for faster training"
      ],
      "metadata": {
        "id": "3lBKG9GLV_d5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also set up. regular checkpoints to save progress.\n",
        "\n",
        "We can set up progress monitoring using wandb (Weights & Biases), which is a popular tool for tracking machine learning experiments. It creates nice visualizations of your training metrics (like loss over time), GPU usage, etc. When you run ML training, it sends the data to their website where you can view it in nice dashboards. If you want to use it, you'll need a wandb API key.\n",
        "\n",
        "Iif you don't want to use it, you can set the `use_wandb` parameter at the top of this next cell to = `False`"
      ],
      "metadata": {
        "id": "CStSBrmLWcNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_wandb = False\n",
        "\n",
        "# Clear any existing accelerator state\n",
        "AcceleratorState._reset_state()\n",
        "\n",
        "# Initialize accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "report_to = \"none\"\n",
        "if use_wandb:\n",
        "  # Initialize wandb\n",
        "  wandb.init(\n",
        "      project=\"llama-domain-training\",\n",
        "      name=\"domain-knowledge-run\",\n",
        "      config={\n",
        "          \"model\": \"LLaMa-3.2-1B\",\n",
        "          \"dataset_size\": len(training_dataset),\n",
        "          \"chunk_size\": 512\n",
        "      }\n",
        "  )\n",
        "  report_to = 'wandb'\n",
        "\n",
        "# Training arguments with accelerator config\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./domain_trained_model\",\n",
        "    learning_rate=1e-4,              # Small learning rate\n",
        "    per_device_train_batch_size=4,   # Start smaller, we can adjust\n",
        "    gradient_accumulation_steps=8,   # Add gradient accumulation\n",
        "    num_train_epochs=8,\n",
        "    bf16=True,                      # Enable mixed precision\n",
        "    logging_steps=1,                # Log every step so we can monitor\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"adamw_8bit\",            # Use 8-bit optimizer\n",
        "    weight_decay=0.01,             # Add weight decay\n",
        "    warmup_steps=10                # Add warmup steps\n",
        ")"
      ],
      "metadata": {
        "id": "sCTmLl7o2f89"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) PEFT Configuration\n",
        "\n",
        "Parameter Efficient Fine-Tuning (PEFT) lets us fine-tune LLaMA using much less memory. We'll use LoRA (Low-Rank Adaptation), which is particularly effective for LLMs."
      ],
      "metadata": {
        "id": "P2puurnJqF-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Toggle for full fine-tuning\n",
        "use_peft = True\n",
        "\n",
        "if use_peft:\n",
        "    # LoRA configuration\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,                     # Rank of update matrices\n",
        "        lora_alpha=32,           # Alpha parameter for LoRA scaling\n",
        "        lora_dropout=0.05,       # Dropout probability for LoRA layers\n",
        "        target_modules=[         # Which modules to apply LoRA to\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        ],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"    # For causal language modeling\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()  # Shows % of parameters being trained"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_eWEkLWqFV0",
        "outputId": "81c942fa-dbda-4279-f7aa-c54b98110b20"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop\n",
        "\n",
        "Now we'll set up the trainer and start training. We'll include:\n",
        "- A simple progress callback to monitor training\n",
        "- Basic error handling\n",
        "- Checkpoint saving"
      ],
      "metadata": {
        "id": "ayhnUHrFMBzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class ProgressCallback(TrainerCallback):\n",
        "    \"\"\"Simple callback to print progress during training\"\"\"\n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        print(f\"\\nStarting epoch {state.epoch + 1}/{args.num_train_epochs}\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            # Handle the loss value more carefully\n",
        "            loss = logs.get('loss', 'N/A')\n",
        "            if isinstance(loss, (float, int)):\n",
        "                print(f\"Step {state.global_step}: Loss = {loss:.4f}\")\n",
        "            else:\n",
        "                print(f\"Step {state.global_step}: Loss = {loss}\")\n",
        "\n",
        "# Set up trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=training_dataset,\n",
        "    callbacks=[ProgressCallback()]\n",
        ")"
      ],
      "metadata": {
        "id": "5WzBd5OpMCib"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monitoring Training\n",
        "\n",
        "You can monitor the training in several ways:\n",
        "1. Direct console output showing loss every 10 steps\n",
        "2. Weights & Biases dashboard (wandb.ai) showing:\n",
        "   - Training loss over time\n",
        "   - Learning rate schedule\n",
        "   - GPU memory usage\n",
        "   - Training speed\n",
        "3. Model checkpoints saved after each epoch\n",
        "\n",
        "The training may take some time depending on your GPU. You'll see regular updates on:\n",
        "- Current epoch\n",
        "- Current step\n",
        "- Loss value\n",
        "- Any potential issues"
      ],
      "metadata": {
        "id": "N4X2jsrqMNZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with detailed error tracking\n",
        "try:\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    # Track where we are in training\n",
        "    current_step = 0\n",
        "    try:\n",
        "        trainer_output = trainer.train()\n",
        "    except Exception as train_error:\n",
        "        print(\"\\nError during trainer.train():\")\n",
        "        print(f\"Step when error occurred: {current_step}\")\n",
        "        print(f\"Error type: {type(train_error)}\")\n",
        "        print(f\"Error message: {str(train_error)}\")\n",
        "        # Print the full error traceback\n",
        "        import traceback\n",
        "        print(\"\\nFull error traceback:\")\n",
        "        print(traceback.format_exc())\n",
        "        raise  # Re-raise the error to see full stack trace\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "    # Save the final model\n",
        "    trainer.save_model(\"./final_model\")\n",
        "    print(\"Model saved to ./final_model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Final error catch:\", str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HLDgRjS3MMdH",
        "outputId": "7ee93aab-28d1-4efb-dd3b-a220260ef541"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "Starting epoch 1/8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 01:00, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.807800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.563400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.944300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.321900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.537400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.461000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.366800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.180300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.334200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.317500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.212500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.239500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.150600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.255400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.138600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.031900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.156600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.197600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.972600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.052500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.062600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.136100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.115200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.078300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.919500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.045100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.088600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.957200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.944600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.014900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.911200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.050600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.867900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.905600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>2.042400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>2.029600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.026300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Loss = 2.8078\n",
            "Step 2: Loss = 2.5634\n",
            "Step 3: Loss = 2.8730\n",
            "Step 4: Loss = 2.9443\n",
            "Step 5: Loss = 2.3219\n",
            "\n",
            "Starting epoch 2.0/8\n",
            "Step 6: Loss = 2.5374\n",
            "Step 7: Loss = 2.4610\n",
            "Step 8: Loss = 2.3668\n",
            "Step 9: Loss = 2.1803\n",
            "Step 10: Loss = 2.3342\n",
            "\n",
            "Starting epoch 3.0/8\n",
            "Step 11: Loss = 2.3175\n",
            "Step 12: Loss = 2.2125\n",
            "Step 13: Loss = 2.2395\n",
            "Step 14: Loss = 2.1506\n",
            "Step 15: Loss = 2.2554\n",
            "\n",
            "Starting epoch 4.0/8\n",
            "Step 16: Loss = 2.1386\n",
            "Step 17: Loss = 2.0319\n",
            "Step 18: Loss = 2.1566\n",
            "Step 19: Loss = 2.1350\n",
            "Step 20: Loss = 2.1976\n",
            "\n",
            "Starting epoch 5.0/8\n",
            "Step 21: Loss = 1.9726\n",
            "Step 22: Loss = 2.0525\n",
            "Step 23: Loss = 2.0626\n",
            "Step 24: Loss = 2.1361\n",
            "Step 25: Loss = 2.1152\n",
            "\n",
            "Starting epoch 6.0/8\n",
            "Step 26: Loss = 2.0783\n",
            "Step 27: Loss = 1.9195\n",
            "Step 28: Loss = 2.0451\n",
            "Step 29: Loss = 2.0886\n",
            "Step 30: Loss = 1.9572\n",
            "\n",
            "Starting epoch 7.0/8\n",
            "Step 31: Loss = 1.9446\n",
            "Step 32: Loss = 2.0149\n",
            "Step 33: Loss = 1.9112\n",
            "Step 34: Loss = 2.0250\n",
            "Step 35: Loss = 2.0506\n",
            "\n",
            "Starting epoch 8.0/8\n",
            "Step 36: Loss = 1.8679\n",
            "Step 37: Loss = 1.9056\n",
            "Step 38: Loss = 2.0424\n",
            "Step 39: Loss = 2.0296\n",
            "Step 40: Loss = 2.0263\n",
            "Step 40: Loss = N/A\n",
            "\n",
            "Training completed!\n",
            "Model saved to ./final_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking our Fine-Tuning Results!\n",
        "\n",
        "If the above cell ran, then congrats! You've fine-tuned a LLaMa 3.2 1B model on some new domain knowledge! Let's test that it actually learned what we wanted it to.\n",
        "\n",
        "We'll test the model's knowledge by:\n",
        "1. Asking domain-specific questions\n",
        "2. Comparing responses between original and fine-tuned models\n",
        "3. Looking for improvements in accuracy and detail"
      ],
      "metadata": {
        "id": "UjcoP3JdQFk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's just confirm that we've *actually* changed the model."
      ],
      "metadata": {
        "id": "7aDrQlPxrGBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "fine_tuned_path =\"./final_model\"\n",
        "\n",
        "if use_peft:\n",
        "    fine_tuned_path = \"./final_model\"\n",
        "    config = PeftConfig.from_pretrained(fine_tuned_path)\n",
        "    fine_tuned_model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        fine_tuned_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # To compare PEFT models, we need to look at the LoRA weights\n",
        "    print(\"\\nLoRA Adapter Information:\")\n",
        "    for name, param in fine_tuned_model.named_parameters():\n",
        "        if 'lora' in name:  # Only look at LoRA parameters\n",
        "            print(f\"Found LoRA weights in {name}\")\n",
        "            print(f\"Non-zero parameters: {torch.sum(param != 0).item()}\")\n",
        "\n",
        "    # The base parameters should be identical (that's the point of PEFT!)\n",
        "    print(\"\\nBase parameters are identical?:\", torch.allclose(\n",
        "        next(base_model.parameters()),\n",
        "        next(fine_tuned_model.base_model.parameters())\n",
        "    ))\n",
        "else:\n",
        "    fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "        fine_tuned_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Compare some weights to see if they're actually different\n",
        "    base_params = next(base_model.parameters())\n",
        "    fine_tuned_params = next(fine_tuned_model.parameters())\n",
        "\n",
        "    print(\"Are the models identical?\", torch.allclose(base_params, fine_tuned_params))"
      ],
      "metadata": {
        "id": "hHnEgQhItepe",
        "outputId": "63572608-d1e8-4074-e5fd-67b1c6adf658",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LoRA Adapter Information:\n",
            "Found LoRA weights in base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
            "Non-zero parameters: 8192\n",
            "Found LoRA weights in base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
            "Non-zero parameters: 32768\n",
            "Found LoRA weights in base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
            "Non-zero parameters: 131072\n",
            "Found LoRA weights in base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
            "Non-zero parameters: 32768\n",
            "\n",
            "Base parameters are identical?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also double-check that the training generated the outputs that we expect."
      ],
      "metadata": {
        "id": "lpSXp7xdrKv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at training output directory\n",
        "print(\"Training output contents:\")\n",
        "if os.path.exists(\"./domain_trained_model\"):\n",
        "    print(\"\\ndomain_trained_model directory contains:\")\n",
        "    for item in os.listdir(\"./domain_trained_model\"):\n",
        "        print(f\"- {item}\")\n",
        "        if os.path.isdir(f\"./domain_trained_model/{item}\"):\n",
        "            print(f\"  Contains: {os.listdir(f'./domain_trained_model/{item}')}\")\n",
        "\n",
        "# If we have a trainer_state.json, let's look at it\n",
        "import json\n",
        "if os.path.exists(\"./domain_trained_model/checkpoint-3/trainer_state.json\"):\n",
        "    with open(\"./domain_trained_model/checkpoint-3/trainer_state.json\", 'r') as f:\n",
        "        state = json.load(f)\n",
        "    print(\"\\nTraining history:\")\n",
        "    print(state.get('log_history', []))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvkYZFjdlV5I",
        "outputId": "fe8b129d-4d3a-4bd2-9715-d6525f4b9341"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training output contents:\n",
            "\n",
            "domain_trained_model directory contains:\n",
            "- checkpoint-20\n",
            "  Contains: ['trainer_state.json', 'adapter_model.safetensors', 'README.md', 'training_args.bin', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'adapter_config.json']\n",
            "- checkpoint-25\n",
            "  Contains: ['trainer_state.json', 'adapter_model.safetensors', 'README.md', 'training_args.bin', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'adapter_config.json']\n",
            "- checkpoint-15\n",
            "  Contains: ['trainer_state.json', 'adapter_model.safetensors', 'README.md', 'training_args.bin', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'adapter_config.json']\n",
            "- runs\n",
            "  Contains: ['Jan07_21-32-59_b043f44b81fb', 'Jan07_21-31-01_b043f44b81fb', 'Jan07_21-30-23_b043f44b81fb']\n",
            "- checkpoint-35\n",
            "  Contains: ['trainer_state.json', 'adapter_model.safetensors', 'README.md', 'training_args.bin', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'adapter_config.json']\n",
            "- checkpoint-30\n",
            "  Contains: ['trainer_state.json', 'adapter_model.safetensors', 'README.md', 'training_args.bin', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'adapter_config.json']\n",
            "- checkpoint-40\n",
            "  Contains: ['trainer_state.json', 'adapter_model.safetensors', 'README.md', 'training_args.bin', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'adapter_config.json']\n",
            "- checkpoint-5\n",
            "  Contains: ['trainer_state.json', 'adapter_model.safetensors', 'README.md', 'training_args.bin', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'adapter_config.json']\n",
            "- checkpoint-10\n",
            "  Contains: ['trainer_state.json', 'adapter_model.safetensors', 'README.md', 'training_args.bin', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'adapter_config.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we'll set up a function to query our model with a question."
      ],
      "metadata": {
        "id": "9yR1k4y-QZ7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Original vs Fine-Tuned Models\n",
        "\n",
        "Now, we'll compare the performance of our fine-tuned model vs. the original model on some questions that are meant to test the model's domain knowledge."
      ],
      "metadata": {
        "id": "GDLmJEyzRfwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Questions\n",
        "\n",
        "Let's define some questions to test our models' knowledge. These should be specific to your PDF content. These questions are for a dataset of PDF's that discuss \"dialogic reading\""
      ],
      "metadata": {
        "id": "ku1hcnOTQwWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test questions based on PDF content\n",
        "test_questions = [\n",
        "    \"Who developed the concept of dialogic reading\",\n",
        "    \"What ages is dialogic reading appropriate for?\",\n",
        "    \"What are dialogic reading prompt types?\",\n",
        "]"
      ],
      "metadata": {
        "id": "Z0qDaogCQuZf"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original Model\n",
        "\n",
        "First, let's look at how LLaMa 3.2 performs on these questions out of the box, before training to gain any additional knowledge."
      ],
      "metadata": {
        "id": "ELI90f0QL9KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline with LLaMA 3.2 original\n",
        "pipe_original = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"meta-llama/Llama-3.2-1B\",\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "# Test each question\n",
        "print(\"Testing original model responses:\")\n",
        "print(\"-\" * 50)\n",
        "for question in test_questions:\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    result = pipe_original(question, max_length=100)\n",
        "    print(f\"A: {result[0]['generated_text']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYZTQAfYT6gI",
        "outputId": "bab5257f-2a78-48c5-cbd8-c1e3294af3ad"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing original model responses:\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: Who developed the concept of dialogic reading\n",
            "A: Who developed the concept of dialogic reading?\n",
            "Who developed the concept of dialogic reading?\n",
            "The concept of dialogic reading was first developed by Jean Piaget in his book The Language and Thought of the Child. He believed that the child’s language development was a result of their interactions with others and that the child’s thinking was also influenced by their interactions with others.\n",
            "Who is the father of dialogic reading?\n",
            "The father of dialogic reading is Jean Piaget, who developed the concept of dialog\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: What ages is dialogic reading appropriate for?\n",
            "A: What ages is dialogic reading appropriate for? What is the purpose of dialogic reading?\n",
            "What is the difference between a reader and a reader?\n",
            "How can I find books that are appropriate for my students?\n",
            "What is the difference between a reader and a reader?\n",
            "What is the difference between a reader and a reader?\n",
            "What is the difference between a reader and a reader?\n",
            "How can I find books that are appropriate for my students?\n",
            "How can I find books that are appropriate for my students?\n",
            "How\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: What are dialogic reading prompt types?\n",
            "A: What are dialogic reading prompt types? 2.4.1.3\n",
            "The dialogic reading prompt types are the types of dialogic reading prompts that the author can use to prompt readers to read a particular section of the text. The types of dialogic reading prompts are: 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our Fine-Tuned Model\n",
        "\n",
        "Now, we give those same questions to our model, which we've fine tuned for additional domain knowledge. We'll look for signs that the domain knowledge has been absorbed by the model by how it answers the question."
      ],
      "metadata": {
        "id": "yCpy4-HNMLc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline with our fine-tuned model\n",
        "\n",
        "if use_peft:\n",
        "  peft_model = PeftModel.from_pretrained(\n",
        "      base_model,\n",
        "      fine_tuned_path,\n",
        "      torch_dtype=torch.float16,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "\n",
        "  # Get the merged model\n",
        "  fine_tuned_model = peft_model.merge_and_unload()  # This combines PEFT and base weights\n",
        "\n",
        "  pipe_finetune = pipeline(\n",
        "      \"text-generation\",\n",
        "      model=fine_tuned_model,  # base model + peft weights\n",
        "      tokenizer=tokenizer,\n",
        "      torch_dtype=torch.float16,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "\n",
        "else:\n",
        "  pipe_finetune = pipeline(\n",
        "      \"text-generation\",\n",
        "      model=fine_tuned_path,  # Path to our saved fine-tuned model\n",
        "      tokenizer=tokenizer,\n",
        "      torch_dtype=torch.float16,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "# Test each question\n",
        "print(\"Testing fine-tuned model responses:\")\n",
        "print(\"-\" * 50)\n",
        "for question in test_questions:\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    result = pipe_finetune(question, max_length=100)\n",
        "    print(f\"A: {result[0]['generated_text']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYKSBMDyfedt",
        "outputId": "b72ba0c5-d129-4b0a-e0ee-097572f64131"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing fine-tuned model responses:\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: Who developed the concept of dialogic reading\n",
            "A: Who developed the concept of dialogic reading?\n",
            "Dialogic reading is a method of reading that uses questions and answers to engage children in shared book reading. It has been shown to have a number of positive effects on children’s language skills and school readiness. Early intervention with dialogic reading has been shown to improve children’s language skills and to be an effective method for promoting shared book reading in the home. The following information is presented: a. The following information is presented: a. 1.\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: What ages is dialogic reading appropriate for?\n",
            "A: What ages is dialogic reading appropriate for? Dialogic reading is a form of shared book reading that involves a parent or caregiver and a child in a one-on-one interactive dialogue. Shared book reading is a well established and powerful strategy for promoting language development in children. Shared book reading is a well established and powerful strategy for promoting language development in children. Shared book reading is a well established and powerful strategy for promoting language development in children. Shared book reading is a well established and powerful strategy for promoting\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: What are dialogic reading prompt types?\n",
            "A: What are dialogic reading prompt types? What are dialogic reading prompt types? What are dialogic reading prompt types? What are dialogic reading prompt types? What are dialogic reading prompt types? What are dialogic reading prompt types? What are dialogic reading prompt types? What are dialogic reading prompt types? What are dialogic reading prompt types? What are dialogic reading prompt types? What are dialogic reading prompt types? What are dialogic reading prompt types? What are dialog\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking General Knowledge Retention\n",
        "\n",
        "Often times for very small models, training them to gain specific domain knowledge leads to forgetting existing knowledge.\n",
        "\n",
        "Here, we'll check a few basic knowledge questions to see if the model has retained understanding."
      ],
      "metadata": {
        "id": "BxCiK5NIOccn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test questions based on PDF content\n",
        "test_questions = [\n",
        "    \"What does a zebra look like?\",\n",
        "    \"What's the difference between a lake and a pond?\",\n",
        "    \"2 + 2 = ?\",\n",
        "]"
      ],
      "metadata": {
        "id": "MsIdhnW-Oabi"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test each question\n",
        "print(\"Testing original model responses:\")\n",
        "print(\"-\" * 50)\n",
        "for question in test_questions:\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    result = pipe_original(question, max_length=100)\n",
        "    print(f\"A: {result[0]['generated_text']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9KM9YoYOj7u",
        "outputId": "b6772c8e-936e-407b-a932-21fe572faecd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing original model responses:\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: What does a zebra look like?\n",
            "A: What does a zebra look like? It is a large mammal, with a striped pattern on its body. It has a long body, with a short tail. It has a long neck, and a long head. It has a long, slender body, with a short tail. It has a long, slender body, with a short tail. It has a long neck, and a long head. It has a long body, with a short tail. It has a long neck,\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: What's the difference between a lake and a pond?\n",
            "A: What's the difference between a lake and a pond? A lake is large and deep, and a pond is small and shallow. Both have water, but a lake is bigger and has more water. A lake is usually surrounded by land, while a pond is usually surrounded by water.\n",
            "What's the difference between a lake and a pond? A lake is large and deep, and a pond is small and shallow. Both have water, but a lake is bigger and has more water. A lake\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: 2 + 2 = ?\n",
            "A: 2 + 2 = ? (2 + 2 =?)\n",
            "2 + 2 =? (2 + 2 =?)\n",
            "Post by dave » Sun May 22, 2011 2:07 pm\n",
            "This is a great puzzle. I would like to see a solution, but I think I know the answer. I think the answer is 4.\n",
            "The first number is 2. The second number is 2. The sum of the two numbers is \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test each question\n",
        "print(\"Testing fine-tuned model responses:\")\n",
        "print(\"-\" * 50)\n",
        "for question in test_questions:\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    result = pipe_finetune(question, max_length=100)\n",
        "    print(f\"A: {result[0]['generated_text']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqpTcsOqOmPo",
        "outputId": "76082fda-0270-4f92-9037-544a7b282511"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing fine-tuned model responses:\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: What does a zebra look like?\n",
            "A: What does a zebra look like? Zebra facts and information. Learn the following information: • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: What's the difference between a lake and a pond?\n",
            "A: What's the difference between a lake and a pond? A lake is a body of water that is larger than 2.5 acres. A pond is a body of water that is smaller than 2.5 acres. A pond is a body of water that is smaller than 2.5 acres. A pond is a body of water that is smaller than 2.5 acres. A pond is a body of water that is smaller than 2.5 acres. A pond is\n",
            "--------------------------------------------------\n",
            "\n",
            "Q: 2 + 2 = ?\n",
            "A: 2 + 2 = ? 2 + 2 =? 2 + 2 =? 2 + 2 =? 2 + 2 =? 2 + 2 =? 2 + 2 =? 2 + 2 =? 2 + 2 =? 2 + 2 =? 2 + 2 =? 2 + 2 =? 2 + 2 =? 2 + 2 =? 2\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
