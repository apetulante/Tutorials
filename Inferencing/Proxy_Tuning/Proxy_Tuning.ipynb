{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/apetulante/Tutorials/blob/master/Concepts/Proxy_Tuning/Proxy_Tuning.ipynb)\n"
      ],
      "metadata": {
        "id": "O1Ku60th1jwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follows example in this tutorial: https://lightning.ai/lightning-ai/studios/improve-llms-with-proxy-tuning\n",
        "\n",
        "From this OG paper: https://arxiv.org/pdf/2401.08565"
      ],
      "metadata": {
        "id": "cU4PwHriVYwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning by **Proxy**\n",
        "\n",
        "Proxy tuning was a concept introduced by Liu et al. (2024). It's a simple, clever way to produced fine tuned-like results for a larger model, *without ever having to fine-tune the larger model*. Instead, you use a fine-tuned version of a smaller model as a \"proxy\", to show the larger model where its errors are.\n",
        "\n",
        "This method can save considerable computational resources and time."
      ],
      "metadata": {
        "id": "sjt4r_5YVfEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Background and Introduction\n",
        "\n",
        "In this notebook, we will implement and demonstrate the concept of proxy-tuning using LLaMA models. Proxy-tuning involves using a smaller, fine-tuned model (expert) and its untuned counterpart (anti-expert) to guide the output of a larger, untuned base model *without modifying its weights*. This approach allows us to leverage the strengths of a fine-tuned model to influence a larger model's behavior.\n",
        "\n",
        "### Why Proxy-Tuning Works\n",
        "\n",
        "#### Logits as Raw Predictions\n",
        "\n",
        "Logits are the raw, unnormalized scores produced by a model for each possible output token. They contain rich information about the model's confidence in its predictions.\n",
        "\n",
        "#### Behavior of Expert and Base Models\n",
        "\n",
        "- The expert model is fine-tuned on a specific task and has learned to generate high-quality outputs for that task. Its logits reflect this specialized knowledge.\n",
        "- The base model, while more powerful, is more general and not fine-tuned for the specific task. Its logits reflect a broader understanding but might lack task-specific refinements.\n",
        "\n",
        "#### Adjusting Logits\n",
        "\n",
        "By subtracting the logits of the base model from the logits of the expert model, we get a difference that highlights the task-specific adjustments made by the expert model. This difference captures the fine-tuning adjustments. Adding this difference to the logits of the target model (which is similar to the base model but possibly larger) steers the target model's predictions towards those of the expert model.\n",
        "\n",
        "#### Mathematical Intuition\n",
        "\n",
        "Let $ L_{\\text{anti-expert}} $ be the logits of the anti-expert model.\n",
        "Let $ L_{\\text{expert}} $ be the logits of the expert model.\n",
        "Let $ L_{\\text{target}} $ be the logits of the target model.\n",
        "\n",
        "The proxy-tuned logits are computed as:\n",
        "\n",
        "  $\n",
        "  L_{\\text{proxy-tuned}} = L_{\\text{target}} + (L_{\\text{expert}} - L_{\\text{anti-expert}})\n",
        "  $\n",
        "\n",
        "This formula adjusts $ L_{\\text{target}} $ by incorporating the task-specific modifications captured in $ L_{\\text{expert}} - L_{\\text{anti-expert}} $.\n",
        "\n",
        "And it really is just that simple!\n"
      ],
      "metadata": {
        "id": "USRn7O6wnAPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n",
        "\n",
        "We'll be making use of huggingface transformers for this tutorial. Llama 2 7B and 13B are both available on huggingface for us to download and use, as are their respective chat-trained versions.\n",
        "\n",
        "First, you'll need to go to (just one) of the repos and agree to share your contact information to gain access:\n",
        "- 7B repo: https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
        "- 7B (chat version) repo: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "- 13B repo: https://huggingface.co/meta-llama/Llama-2-13b-hf"
      ],
      "metadata": {
        "id": "MF2coS0GVf_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll begin by installing some additional packages. This cell might take a moment to run, and you may need to restart the kernel once its finished to be sure you have use of the packages."
      ],
      "metadata": {
        "id": "fxFclFjTnaHK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFnNBzA5VT0Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# we don't need to see package install messages\n",
        "!pip install accelerate datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, you'll need your huggingface token to access the repos.\n",
        "\n",
        "If you need help finding your token or making one, check out this tutorial: https://huggingface.co/docs/hub/security-tokens"
      ],
      "metadata": {
        "id": "agY0dTzJgm9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# This will prompt you to enter your Hugging Face token\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "e6577ea98f834402bb95d88f1601e5b4",
            "96204e265445412abd7fa6303641b2f1",
            "6a7f4b5b513e4510b1902fcd94658851",
            "d8e9452af1c144aaa5927eb5046479a1",
            "78a373d043904b0f9d62dbb1ff7cdc74",
            "0d149bdda3864797b0b97bea679a906e",
            "d86a91178a424d0a871cf239c3059c5a",
            "afe40103d5044366860475d97a9925d8",
            "423c5237c55c4ababb1cabf4f795b389",
            "22fc0517bb6c4657ae1ae12af374f386",
            "6b04063386184041be90a9d3cc873d47",
            "60e99e1a8cc04cb3b99c8e6456e5cb0a",
            "c1dd72bd8ebd4553a0b923b050bd05a5",
            "5f7239386db747559f33ceb33248121a",
            "05d666c331af4e258841a5c210e16f42",
            "f877fca1371546649ffee8a0e56462e8",
            "4c24e5885019478f978ed04b2f5a70da",
            "99be839389664bcbaf290b14a9654ffc",
            "6377f30b72384c9ba700ff9399c9f95f",
            "c4997df28a0744c382b23faa29fd45ef",
            "b92a0b3eab10442db13a4ce95b67859d",
            "816df23850684af5b10765e9d3f74b2f",
            "b985f323907f4e95b011f9947cadcee0",
            "46229f50e722435096b7a20521ce564d",
            "c982ae9d9d7c48d596e6e3198368260a",
            "86f382f15b954f5ebbefd8d59d91bfbf",
            "5e55264bfbb94f77a6e914d127ba78d5",
            "48a8df3dd4fd4434acc70149ccba702a",
            "6b188569c9054c339cc2eac566205b01",
            "3f62f3a1a3e64144b5d15264387446f7",
            "b4a823527c2944d8bbc2f94251f087d7",
            "efe3d69df07343d4853c12d00e6902ca"
          ]
        },
        "id": "aAt5JZubffvL",
        "outputId": "e3bda2a8-b852-418a-fe3d-2b9f6d729a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6577ea98f834402bb95d88f1601e5b4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load in and Test LLaMA Models\n",
        "\n",
        "We'll start by taking a closer look at the models we'll be working with.\n",
        "\n",
        "In the interest of saving compute and time, we'll be working with Llama 2 models, both 7-billion parameter and 13-billion parameter versions.\n",
        "\n",
        "Let's start by loading our 3 models. Llama 7b-chat will be our **expert model**. It's been fine-tuned to answer questions.\n",
        "Llama-7b is our **anti-expert**, it's like our expert, but without the behavior that we desire. Llama 13b will be the model that we'll proxy-tune to improve, aiming to get its performance closer to Llama 13b-chat."
      ],
      "metadata": {
        "id": "4iJh34m0qQyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "\n",
        "# Load the pipelines for the models\n",
        "pipeline_7b_chat = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                            model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
        "\n",
        "pipeline_7b = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\",\n",
        "                       model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
        "\n",
        "pipeline_13b = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-13b-hf\",\n",
        "                        model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
        "\n",
        "print('Models loaded and ready!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7a899c8d625b4d5781931a81ff9c98e5",
            "749cf718b2e14df4a7cbb2ca2be33ee9",
            "1b50894029d742e786ef803416369676",
            "41cbc904366149a8ba1a03ebc87ada30",
            "3c946b24dec84f1bb4e4721bfad7a127",
            "d17044054f124e3f891cacd13e565b6d",
            "5c79d2b9130b47bd9b5dc739674b8314",
            "2465a56789ad4a6588748b4bcd2fd65f",
            "a6c9ec891db043ce8e71434a1cc2befa",
            "9085289213ae4164afd50649f29f3f98",
            "572208078f51485db670e5c03c2e01b1",
            "ab1c8b7f21a84b468f82f4976d214178",
            "05055ed7e91a45448201297a81a69d1c",
            "02e3544f915b4c4883ddef45263508f7",
            "4894632affe4496fbc07c5f2eaed7242",
            "0945ed4a9e8447dd87b13d80ffa535d5",
            "ed7a111dc3694c1b941704d03f5e85e0",
            "045c3ef6aee44bf498b4bc56b323d0e8",
            "2ac43a5b74c140d5a820292ed02ba557",
            "349026cfe19c43bcbf8455d35d577c8b",
            "057c5d269c1541df84781ed45d578f9d",
            "e1593fae9f2f4927998b8acfffe57cb0",
            "13ae2d0fdb994eb9a2b5c585cd19c09f",
            "63257f15bf7442e99cdd52168cf5e07d",
            "542e97497da743ce883e13b8c6a81412",
            "ffee85f146164c0687cd13563e755980",
            "c1eef87faee649f69dd5d18c880551b6",
            "1e95d508c02a47de8a633bb1c605035f",
            "2d403f6acc5b4118bc6f15190bdc89bc",
            "03d695259990490c890200caa6e66c91",
            "c6ea9197586b4b9bbf72e8977ac2f8e3",
            "6ecc1b70783346f7a78d3d0d6e7e84c6",
            "594faa8772584e2bb0ff97e42797bfdf",
            "257beb9be084481cacc8a358fe5b20d2",
            "6f4a95a06c064d0abebe85f200cf9ca3",
            "9d67ad627ef74d6ca2e4fd8630d89e92",
            "1045ab9a5194495bac5daf435a3d4bae",
            "4c8bf1fda15c49608b2409672612441c",
            "fce089e93ed248b7ae39ca0fd41325d0",
            "c313c209c5a74bebb75038392335e2ed",
            "c24cd239427844808141ed8d0a2f1ea4",
            "4cb8653a586a40a7b116379df37de0a8",
            "96b0e82e75dd48429081e41cef8df4e7",
            "84d502492ffa46e08e84965c57dd65ab",
            "3dbe789f03ea45b9b7a1ca7b6aef3ff6",
            "8479bcb833024fcfbfdcd6f0074df7c5",
            "20ebb9325c7e47c1a0c439ca3769a311",
            "71a56042102d48da977a4c980b274363",
            "aee5bd5e189a49d6806bd79d784c9c33",
            "8f2ae62611af4bc2b7f03a626a1712cf",
            "7652257b2499420d8ee094ad951341e5",
            "6a386ac1af0a444a9445df1aa3f6c158",
            "cdade0fff0af454ea899cd3db660503b",
            "26e2da2c31764c94973d0ebd98aa8886",
            "6aa2d99f84ca4e3595e74148dfdac60e",
            "79261857a78c45b0b9d7745886f8b087",
            "a8f3f595ac1d4780a069ca5ebddd196c",
            "8a54883f2619426281752eb51930cfad",
            "3247e7d47e6e4f678ec89eb5df6c190c",
            "1e15b60947984164b2f7ace5fc4d9d8a",
            "34a619bf906247a3a56d507a86993b91",
            "b9aa66bf6d074293b40db0ae072ee269",
            "ccddcd6dc6144b16b66eaf1324ec3110",
            "ddf54500514743d9b576d974f715778b",
            "a1499645ccf3447aba648ef3be91aa6e",
            "431096c0a91a428983f40ed6ab4bbf78",
            "4e5e4607a2904f8c83a8ab2f6e914a75",
            "f2942178903940979760ef6679bfac3d",
            "383f504b2d904c4590e7b48f2e70b778",
            "e252639d566b43a698357b425ccf9748",
            "accd2d19c48746e0b64f02cddbb8d9b4",
            "43de5b3a43d44b7895b06dcaa7184444",
            "752b8957cb89423f8712c80e602a0b8e",
            "a897f609468a4d80b274f06406075124",
            "695acdd4a5584134b6d6a390f45041b2",
            "395f894e7d4f481a8dd7fb6738091046",
            "e43442cd28654d68a0022ab48b257938",
            "eef04dce208a44cfa68e19b24150a91f",
            "a7a40bba44094986a0840ad617c2d8f6",
            "8abd686ec0794bee80bf566f74902f0e",
            "ada16f04dd144c149df12dd21eb5b561",
            "058f5ff0e58547a1ba035246e2b0c18f",
            "bfe85c3200db423d8f0f03384ed88c31",
            "6f6d6286b9344d12a77702b01ffb4e61",
            "82192d2489e34dfeb4816ac126e37ddd",
            "639ac2e7d7104da5be5ff0aab9faf8b9",
            "22f2c03b5a3442348c0e37efb22482b8",
            "1cb552c8fe8f4c789d9bf425cb08fcfb",
            "8b9796f235c64ddc989d3407340689e1",
            "0e98c416261148ccb3ebae76a3725c3a",
            "a4c10a3912284b4b92916f1a5169d83e",
            "fd44241ce52f45e2aff2fdf93ab091d4",
            "0739d86b64ce4f569d72f483ce50c53e",
            "f6c0f64ada004e5dba1bc8b255b2fc44",
            "f035a3edbfeb4a4c96c27663cd83f704",
            "628308c73e0f4479848415bc53b87dba",
            "6b7737f1ee904b0cba205f2b4e67602c",
            "614d1c36524f4194a869e80a22e197b7",
            "2a90d3e77c8a4ac4b22e8b2f3a3bd32c",
            "3bc1f89cb56541699252e36524b4780c",
            "ca74ac0dacdb4a5daae71e527a4fe488",
            "964ee993a4c94676bc95159a65e9b9f0",
            "3cdc17f2f2624c95b906f08af2343cf0",
            "cdca02955beb424b8d1026a6752869c3",
            "ecc87dac86b3442888023ea898f04883",
            "b6ad74b0050f46ecb81d3659e4e7b296",
            "047573a0fd044a42bf4e6c6c011fa9bf",
            "73cc0d954edb41e79ad84a187a6360a5",
            "0a911763c64942e5b7f28364c17b1b5f",
            "cc6e18c74e3f46d69b7c8ccad80d8ab2",
            "b8851af129bb4fc3a3614012a66d48a4",
            "c0d72e0dfb054fc3a54292bde17c71ae",
            "8a5a148bbbd446fdb22e5fad085b6f2c",
            "27d8652df8534531a4a089dc3bca40d8",
            "5dad0d7f8bfd46fea43999ec0962a316",
            "aa3bab61683c490591c24e9138f957ff",
            "4c8ef31f5fd14c04a2cfce3feff64428",
            "d05797565f5f4059bafa870a0cc80da7",
            "f2ef22e40c2a47678a6001e9fd24b431",
            "ce3b355b84b74a0b87de2c2c37797fa6",
            "d332e89e53d84461a10c3ccfb5e01b7c",
            "8bdcff99aa3d4361be7f10dc257f16fb",
            "ec50c2fa516743229fb1fe8352164815",
            "214e87b1978e4914ae6a35aa53da2244",
            "151a379dbf884e88b6a1d01f13e983b3",
            "4d3f4533fd7146af8764d608d974c190",
            "deb378ab4620451591888d8f50a5c441",
            "559dc9f4b5244fc6b9f1e0961774c08e",
            "64c72d4867f5406fb625cbbdc276dc10",
            "e59374ccec384f3a8c528503f6dc2d56",
            "f3b45e5f3e1544c5864dc7bde50509f2",
            "3c60fed401e545a09f08817182dbf6f0",
            "4bfe150a6ea24945a835c6fd9e89357e",
            "724f045def844b7b96e69d48d02f43d9",
            "869c309a52df43dda643e2e0db7080f1",
            "360439db882c4502952644cab8fbde1b",
            "b248304cf30d491f8c009251f3a0d313",
            "072c9af58d824df594ecc926401fc427",
            "485e9cea78044df19ea10a3171d460a9",
            "c9e437c36d7b4c6195e2d3c7ff3a7c1e",
            "3e3715a91f664af5a6d6a3e5e85fa036",
            "048a964e35a04187bd595ce6756ef00a",
            "ee63dbe43b8e444f926da12c2d754be8",
            "ae54921dafca455e993d9826b6dc7b53",
            "2a265069fbe84bc0a053b8bacdeb760a",
            "f24f8b9dfe1140e29429cffea4b8a67e",
            "8761817837a94707ba045f9e54434e49",
            "2ab2de512f2b434ba6bd0573967f860d",
            "429a93ccbbf249a49bc944a7279a3f9b",
            "ef1491057ea54570b239b07fd53b8a08",
            "dbf0e082194940438831a02e1b17ef28",
            "43bc502c50f447298f3caf187ab43849",
            "9b3fc488d2004615bab02909eb8f8371",
            "22462afee4fe4e9aae2b3ba80da408d4",
            "e80f6c9ac668469c837baceae8f852cb",
            "81ea1894f00c4756b1cf4a851d7e8eac",
            "db840836b41f49de97ef3571769887f2",
            "9e9007b5214f47f0866d8085b29ce208",
            "151162f784c4496eac34a9c5751ffeee",
            "ed67ae76910b489ba45a38198b513638",
            "6942c059c3fd4e6fb43b85c88cbb2777",
            "40fd9d1a681046cb95cc63da371bae4e",
            "253b7cebedb84025ac6ebe6a6beef03e",
            "34a31c16b07c4c80ac3e144ca54b591b",
            "aa130cc91b634d3db7c808b7fdd551e1",
            "b9fcbe0a889f453fbc765e8e4b9e4df2",
            "c5fd00cace80433f8202a8eec5efbc7d",
            "8db820812c42412398c4454a297f90b7",
            "16075feb14ae40bead75532c43355180",
            "a3a570222f3744f9937f219c21ca0d5b",
            "5e4eb0fcc422419ca31a7afedbfd9775",
            "e05868ba67c7460c9292fd05465ceacb",
            "841d183cd36340588f1594f6d7c3e1ad",
            "f0d04b23f12645b795a017809b5e32a3",
            "827c174ac51b4429be59cd92cfc4de60",
            "60be770d4bdf48c6b5c16efe6d72549a",
            "d44d68c432974ecd9d413977bff3788b",
            "e861692788ae4dff889fdecc601d19d0",
            "ba84a82df4004367b59d885f66c2c938",
            "1e85082620c94cca8b6d09489a9e0ef1",
            "91ee472ed1ff4f2096a7f1f73050a391",
            "e29a05aa9c9a45fea1d54b52de9c5cf9",
            "2d516308f5e74f7d9d90c1e48caad18d",
            "40dd2652dbb641b39d30a724c99081d7",
            "c110f6f246d7471eb3489d54a2f778a3",
            "d8a141232fcc493d96e934d2a6de2fd4",
            "f548c5c0d5fa47fea444b83efdace118",
            "d60d969c191546b18a71d7aeafed7b02",
            "f2ed645de6dd4bbe8185aa7f7f3c053d",
            "1cefa1e8f2ed4feb98f3edacfa1c1f0f",
            "9f185078485142159e57cf1aff0ec8b6",
            "e75532a4186b48f48620304de2f22c42",
            "f6a424c79a2b4a5f920be793699f63b3",
            "389c0ce35f344827b384c0a8c620456e",
            "8c9ef27641bb47779ed9f437ccbfeb35",
            "1ffa757721dc40079c8c4b1f63eea405",
            "d4f9c049465044beaa417f1b4d28b218",
            "ee1dc9ae63894141b1a843c9ac5cf345",
            "1058c06a994c4ca0a3420e07eb6c941b",
            "6505824e804145439edd5de349dd7038",
            "5a8ed3db384a46a5bcab95b9eb1afb49",
            "c6b173a8b725440797f5ad756383acc8",
            "6ffa675a36f641bba5cd0a08c348f115",
            "6b7eb5e04c614db7b901cd26b06af948",
            "f0fe1d18bc7447fab1d6a51d57a13415",
            "c82e4e6383994bbeba5b8fe969c71374",
            "671146f0fb4f490fb84df6578d47349a",
            "f9b68efc79ff4262ac77ed27d2626fc8",
            "1ce8cc27af584dde9d90ace6a6636e5e",
            "3de54d0a5a634ac18140a88680382d3c",
            "7bd8e22fd8914b4b89d3a2a8c7aaa9c2",
            "6488ab44b5a64cc4ad78628790c6ec55",
            "d884b6e282d64f49bee2c8314b43a34f",
            "c6098fe73e5c441ebfd8fae927004fc4",
            "f5b98076eab6487e9fa7a599ff55c178",
            "b52276f33d50414a9a87471f4d055fae",
            "d38d5bbfb165439f87b2ee57981044b8",
            "496c07014d6247bbbfaa229dbeabacfd",
            "0926dedb5bdb440684445669af17069a",
            "418cf9ba875845d8ab8666d560c44c97",
            "47e91c15b9c64753954ef616e56a53f5",
            "52643d36ce864be5ad91fbe98a76b37a",
            "024e5fd7f1eb452398875a97907a30bd",
            "cf8ff621273d4c34bd864a04021ed7e8",
            "a6e5960c1a484503b0ad880e2da03316",
            "2bb3b04559e44c53b185480da9e19bc1",
            "32aa8445520c4521844b04c77add2d5f",
            "9d3c594aaeca4355ab50029bab69aaff",
            "e2e5f0eac2dc42678dd2f03a4e2c7d78",
            "fc4bae2a814b4b599367c8fe0f5bcfe2",
            "77cdaf16a02c4927b2ad0c9dcf97f31f",
            "22878085fb9b4583962211c79e80befd",
            "1493005863ba4d5e936957c690087a31",
            "dfc019e54d634cd9b4c4c7f35a8f046a",
            "b5866b7b3d484bae9f7202b09f38d3ec",
            "34cd953fd26c4f2781d06143505073f4",
            "e3a3296316274af38448fd10eb782d61",
            "ecca21a1c87f40ce8aa6a17310ff2cd4",
            "9f3d60dfe479458185c89114a6ca4739",
            "bc7bba087f1b4157868f5688594ae276",
            "1d6cff6e0a5249b9a58bebef01c95d49",
            "6b3c9727ab6142f1a1110c0da36e77ef",
            "886acb82905e4273967de4be71d809e2",
            "c81e9d710ea34d668b11b66aaac5b330",
            "a00bb02d3bf042618bcf74c77f43ae17",
            "93755fe447ea4773a77276a495e457a1",
            "5bae2fd2440b41d28c44d9a65caffb11",
            "f1f57f21275645fd86902fcee645a2c3",
            "e6c5be2431df4b69a50cadc2cb117beb",
            "b1108dc1c96d45f1ba8147b3cfc8068c",
            "14911437286c4d5380c0fed23d9cb1af",
            "02a8657388c34c639981af7a40adeec4",
            "9ff1ab440ccc45d394975398803b7603",
            "21632fa6b111438b8dc61d7267f7fe71",
            "1a2c57704ae741ca900d3799f1726eb8",
            "4ce26d5f241340c2a1eb885683cb59f1",
            "1ec907036f1c4930a73ed1b8b521b3e5",
            "758f3e39a744411087f8f1c7661e3f6b",
            "ef90347259f4427983a7c3146a2a7ced",
            "330f47102df04376b4fbbac6226daa38",
            "1c23634d08374b58a04501386b10e40e",
            "9805d0555f3d458eb2bbffd90b82110d",
            "e16bb55553af471ca4423b525d146219",
            "62dcc3328fa04238baf171d779d69d1b",
            "d31a70d1e480442a8204da75569fea1a",
            "b7a6fed5d63547e9b457031c67739c7e",
            "20a3bf64c7d8482693397acf05475846",
            "6d9bf15be77e4b029b340b8c55203bcc",
            "e772c03853ab4e9986043843d534bc5e",
            "ecd8dfb911b342f5a75d5da88ea03d6e",
            "f4c18f37f1464db886d4da63d1bd3d67",
            "33b0a7df55a74639aa9bbe6a02068d75",
            "4fbc84e09b1940d581c0dfea787c3f2c",
            "09ffda654a134cb28d3d62d44b57d7bc",
            "d0a1aa818eb54dc48022d289acf10dfb",
            "ea48e85dac2445c5a8fa444e6718626c",
            "2bfa8e0dd79343ae90e566ecb0f41d65",
            "df8918d81fed42348247ae91d75ca682",
            "6e531fa0ed5840828b845e7dabbf9eb5",
            "079c72818ffb45feb91bc7197b6baabd",
            "d84782c3380f4b91be72e6f2da6bbe85",
            "eabf243857054025947b2ebc6d9e84ce",
            "720b368209ee45e1ab7e7c225801c6b8",
            "8ce19ba4217c4ab993d4e2c59efd6fea",
            "8fdfc220c5d940bf87bd95beb5018b9a",
            "130cfcce6766455ca81290a4eb7167ab",
            "ddbf0b7410ba4283828665d210aacb70",
            "eb2f0c42a0ea4786956d63a5552e8792",
            "8b41e80f27f14f7c9eb40ce009cc8e93",
            "8d927d3f2c204bd5bc1424696b910452",
            "a528159c293f49efa2d8ded5902bf928",
            "2e848c97e0344e53999afb36f3e40643",
            "4c7811abd4fc4f95976b617a0f431737",
            "37a504017e5b44a5bfde3e4eb35d29b8",
            "5e8f35ba74614bc29fc29eb9ebb1b469",
            "280d2c82299b4a00a7245024f3fe4898",
            "78d235563a934f9aa8c467e39313cbc2",
            "263ddcc9e7254a8aab76f7c6d01895aa",
            "c0ef3aca3a8e4841a94e5861192599da",
            "f6c778ffe58c42ceb9959addecb62bce",
            "f7a943b2e70945ebb87bfbb35f265db4",
            "22f45e079c5b401c9a2a105b399f81d2",
            "98a67582bee84d899d434a14705330aa",
            "0bba0971ab7c4eec90b0cc96ebdff7e1",
            "f1e1a18e1a984117971a7f1596c13df1",
            "58b2f2ef355041cc952d2f6a07cfe4c8",
            "be75599e15534d2195fc9ab22504dad9",
            "eecbeca18cb24aaebc342049eed44405",
            "2d185a1fff0d4d3b8a9153682206962e",
            "31b6b48214474e5eaf8f88d100f7131d",
            "933e5dbb75084c1ca7e73e4d80d33fc4",
            "2441aba14e484cf7bb51cdb1c187f42d",
            "0d61ae37da50491aaed2bf30ec6390db",
            "43df83cb9bf449f6b2cd258df41e8e5b",
            "7cab3d1fc23e42fc99edfa5d1ee527c6",
            "722bb7827ce74824a9836880ad173934",
            "8bc6cbfb8bcc4149a226c703b0c6500b",
            "ac0c518a2afe4684bcbdf41f3a89c9b7",
            "569ed29c8c08404c9c6e755a88c5f4bc",
            "6a0020c463974391a75b806beaa6e045",
            "919e466f322e4798bef9819b76f7a17b",
            "29063ca065e141e280557a8fca7b92f8",
            "e21976c85b604264895eb330e177bdd2",
            "4b9a0cd55f9141bab4bddbaa90146381",
            "d63700ee8e76483eaf087d95695618ea",
            "8ed491e48d684b968c8b79c881a869e7",
            "d3f026e9e3f24ec6bb0ae400e98ea2fa",
            "904f86b92e254458937dc7a80efe1a8b",
            "6715e08fdd6a40d2992345cb9f40da63",
            "d6ffa62b965d43ffaf5b00245ee65848",
            "41e57f6dd2254a7b99ac1b3fb96d9847",
            "9a2484d0125e4287b1e1a8541e863e82",
            "0361f4a0c44c45daa5ffdcda5d239520",
            "16627633b1a94d04a29b7bf4fd46400d",
            "fb4c5dab638048fa83210556852788fc",
            "27b713c1b6b54e3697efa7b657b9268a",
            "b222ca99abec4b51bed72fb6eb4d0ba9",
            "62f6351f70774f959dc9a6abcbf5eed8",
            "b5d6465122c749c2b59e0f33437fe124",
            "ab8068326866470bbcee329b212b5417",
            "652a4afd9df9407db69451df3aed8504",
            "bab19ee4a1b44a49a61d461bc2fa0f44",
            "05200a5c935749b18ac13f22f4823ac0",
            "a2f2f935606440ceada9718d154049e7",
            "08e9f153efee4368a94a925276317008",
            "ef53eac7d52b4cd5a722ce4bae9b3b3c",
            "aa6b42e29d524d41b75c33e86fc9c5ef",
            "9485c4054293407f8ee766c3221af009",
            "b50c9237be164c0e8f6bae5ddbbf0cb3",
            "0aa32e2b538143979984123df5ec1f1d",
            "4523ec7fbf7f4febbb756d19cca4ead9",
            "d29aebcf00d8449a83da2549d949a97e",
            "4f6e995bf9964e1ab7f9f0e98975b509",
            "30d4e4f9315c4459b06ca1c94894af6e",
            "caca6f6df917450fa657f42962d2f275",
            "3c7c2ad4d9cd4d0abd2806c1f57cb92f",
            "885dad0bb1444ed4a17fcde42f7083df",
            "7e8e58255416416bbdb08848a0ad1c43",
            "d3bbd2a30db7451ab3c1e83dec8b7002",
            "3ea8c144805447328c216b4650d13a33",
            "f6169ba8b3d044438ad326df37630936",
            "e06709bdb03c4decbe610df5fa200a91",
            "c7fc86321a6b4829ad8bd4834df7de5f",
            "70e7f5dda7054480b01f4cabf8474c38",
            "47c4fe822a8340178f4e436376198e66",
            "8d6c990ebb544d31adeb9be5162214a1",
            "34ff4d4e3f964e7e8f89920a06901566",
            "8ea419f34056457cb1b9488a2438bad1",
            "12e3deb82c824ddaafc27784f3a8c876",
            "5ac25efc3b5b4000beb1cd23b3aadbee",
            "2463cdded1434a80b707a72ed21359e6",
            "c2a29e85db4840ba8bfc136c3cab7f07",
            "a416ed44e5f74aef8cf4516164e5888c",
            "7a3fd0a5551949bfbc3b47eb29befb34"
          ]
        },
        "id": "qAhY_a8DXSm-",
        "outputId": "3623d0b6-244b-44f9-a111-033c975b53c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a899c8d625b4d5781931a81ff9c98e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab1c8b7f21a84b468f82f4976d214178"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13ae2d0fdb994eb9a2b5c585cd19c09f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "257beb9be084481cacc8a358fe5b20d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dbe789f03ea45b9b7a1ca7b6aef3ff6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79261857a78c45b0b9d7745886f8b087"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e5e4607a2904f8c83a8ab2f6e914a75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eef04dce208a44cfa68e19b24150a91f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b9796f235c64ddc989d3407340689e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bc1f89cb56541699252e36524b4780c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8851af129bb4fc3a3614012a66d48a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bdcff99aa3d4361be7f10dc257f16fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bfe150a6ea24945a835c6fd9e89357e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae54921dafca455e993d9826b6dc7b53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e80f6c9ac668469c837baceae8f852cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9fcbe0a889f453fbc765e8e4b9e4df2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d44d68c432974ecd9d413977bff3788b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d60d969c191546b18a71d7aeafed7b02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1058c06a994c4ca0a3420e07eb6c941b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3de54d0a5a634ac18140a88680382d3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47e91c15b9c64753954ef616e56a53f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22878085fb9b4583962211c79e80befd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "886acb82905e4273967de4be71d809e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21632fa6b111438b8dc61d7267f7fe71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d31a70d1e480442a8204da75569fea1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea48e85dac2445c5a8fa444e6718626c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddbf0b7410ba4283828665d210aacb70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "263ddcc9e7254a8aab76f7c6d01895aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d185a1fff0d4d3b8a9153682206962e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a0020c463974391a75b806beaa6e045"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41e57f6dd2254a7b99ac1b3fb96d9847"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bab19ee4a1b44a49a61d461bc2fa0f44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f6e995bf9964e1ab7f9f0e98975b509"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70e7f5dda7054480b01f4cabf8474c38"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what Llama 7B says when it's not chat fine-tuned.\n",
        "\n",
        "We'll ask it a simple math question. We don't expect an \"answer\" here. Remember, that it's going to try to finish what we were saying, so the response can be a huge range of things!"
      ],
      "metadata": {
        "id": "HbYeK8un3fwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = '''If I have 5 apples and eat 2,\n",
        "                  but then find 4 more on my way home,\n",
        "                  how many do I have?'''"
      ],
      "metadata": {
        "id": "Ist1byDYXTI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_7b = pipeline_7b(input_text)[0]['generated_text']\n",
        "print(answer_7b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENFGh-J1XTMA",
        "outputId": "4b4b496c-f408-49f7-d216-f0f4bb31588d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If I have 5 apples and eat 2,\n",
            "                  but then find 4 more on my way home,\n",
            "                  how many do I have?\n",
            "\n",
            "                  ANSWER: 7\n",
            "\n",
            "  */\n",
            "\n",
            "  // Write your code here\n",
            "  var apples = 5;\n",
            "\n",
            "  if (apples == 2) {\n",
            "    console.log(\"I have 7 apples.\");\n",
            "  } else {\n",
            "    console.log(\"I have \" + (apples - 2) + \" apples.\");\n",
            "  }\n",
            "}\n",
            "\n",
            "// Write your code here\n",
            "function countApples(apples) {\n",
            "  if (apples == 2) {\n",
            "    console.log(\"I have 7 apples.\");\n",
            "  } else {\n",
            "    console.log(\"I have \" + (apples - 2) + \" apples.\");\n",
            "  }\n",
            "}\n",
            "\n",
            "countApples(5);\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's see what Llama 7B chat has to say. This will attempt to respond to what we asked, but Llama 7B is a small model, so it might not be the best at the reasoning problem that we've asked."
      ],
      "metadata": {
        "id": "yaWUSoON3oOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_7b_chat = pipeline_7b_chat(input_text)[0]['generated_text']\n",
        "print(answer_7b_chat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ9ZtNl3XTOj",
        "outputId": "1ea6c6b8-1e78-4cd7-dd2f-9fefb6165015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If I have 5 apples and eat 2,\n",
            "                  but then find 4 more on my way home,\n",
            "                  how many do I have?\n",
            "\n",
            "A) 5\n",
            "B) 7\n",
            "C) 8\n",
            "D) 10\n",
            "\n",
            "Answer: B) 7\n",
            "\n",
            "Explanation: You started with 5 apples and ate 2, so you had 5 - 2 = 3 apples left. Then, on your way home, you found 4 more apples, so you now have 3 + 4 = 7 apples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that LLaMA 7b-chat is able to act as an \"expert\" in question answering, because it knows how to answer a question we've asked, whereas the base Llama 7b couldn't."
      ],
      "metadata": {
        "id": "xIFGyXXVu1B9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Proxy Fine-Tuning LLaMA 13B\n",
        "\n",
        "Now, let's proxy tune Llama 13B, leveraging the difference between Llama 7B and Llama 7B-chat to better guide its output."
      ],
      "metadata": {
        "id": "L8zTUCclu27e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Proxy-tuning function\n",
        "def generate_proxy_tuning(pipeline_base, pipeline_tuned, pipeline_target, tokenizer, input_text, max_length):\n",
        "    \"\"\"\n",
        "    Generates text using proxy tuning by combining logits from base, tuned, and target models.\n",
        "\n",
        "    Parameters:\n",
        "    pipeline_base (transformers.pipeline): The base model pipeline.\n",
        "    pipeline_tuned (transformers.pipeline): The tuned (expert) model pipeline.\n",
        "    pipeline_target (transformers.pipeline): The target model pipeline.\n",
        "    tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use for encoding and decoding.\n",
        "    input_text (str): The input text to start the generation.\n",
        "    max_length (int): The maximum length of the generated text.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated text after proxy tuning.\n",
        "    \"\"\"\n",
        "    model_base = pipeline_base.model\n",
        "    model_tuned = pipeline_tuned.model\n",
        "    model_target = pipeline_target.model\n",
        "\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Proxy-tuning:\n",
        "            logits_base = model_base(input_ids).logits\n",
        "            logits_tuned = model_tuned(input_ids).logits\n",
        "            logits_target = model_target(input_ids).logits\n",
        "            logits = (\n",
        "                logits_target.to(device)\n",
        "              + (logits_tuned.to(device) - logits_base.to(device))\n",
        "            )\n",
        "\n",
        "            predictions = torch.softmax(logits[:, -1, :], dim=-1)\n",
        "            next_token_id = torch.argmax(predictions).unsqueeze(0)\n",
        "            generated_tokens.append(next_token_id.item())\n",
        "\n",
        "            # Append the new token to the input sequence for the next iteration\n",
        "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)\n",
        "\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example input text\n",
        "input_text = \"If I have 5 apples and eat 2, but then find 4 more on my way home, how many do I have?\"\n",
        "\n",
        "# Generate text using proxy tuning\n",
        "generated_text = generate_proxy_tuning(\n",
        "    pipeline_base=pipeline_7b,\n",
        "    pipeline_tuned=pipeline_7b_chat,\n",
        "    pipeline_target=pipeline_13b,\n",
        "    tokenizer=tokenizer,\n",
        "    input_text=input_text,\n",
        "    max_length=60\n",
        ")\n",
        "\n",
        "print(\"Proxy-Tuned Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYiChCBuxtek",
        "outputId": "6c910f8c-60d2-4077-a8b9-9d7bf38ef16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proxy-Tuned Generated Text:\n",
            "\n",
            "\n",
            "Answer: \n",
            "You have 5 apples initially, then you eat 2, so you have 5 - 2 = 3 apples left. Then, you find 4 more apples on your way home, so you have 3 + 4 = 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The proxy-tuned generated text should reflect the influence of the fine-tuned expert model (LLaMA 7B Chat) on the larger base model (LLaMA 13B). The output should be coherent and similar to what the expert model would generate, without directly fine-tuning the base model."
      ],
      "metadata": {
        "id": "g03dFIxyo8Hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, we can confirm, by checking how Llama 13B and LLama 13B-chat perform on their own.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "** Note: I ran the following cells by restarting the runtime and running just these cells, to deal with GPU memory constraints.*\n"
      ],
      "metadata": {
        "id": "GAGLTts6yJzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "pipeline_13b = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-13b-hf\",\n",
        "                        model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
        "\n",
        "pipeline_13b_chat = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-13b-chat-hf\",\n",
        "                        model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "a6b2ae9ca3b74b5797e114c97953ad88",
            "cfd108477b3e46d9ba95e13fba3a9280",
            "e1190482afe2429b8232acf2b5494407",
            "96267fe7db944bf1b98fa20ad1bdf3e6",
            "8b57c30de7ab4733a66d2c0ca99a59fa",
            "3b43a34ed6c14741a9a967ff56ad0bce",
            "6797a890005244b88fe73f6debcb7d7d",
            "0ccc783fcb944e0eabac8bdc3ae86616",
            "e6140d7c068f47599c52f08ff70c9ca2",
            "80a5779258474efd97af5679f8b365b6",
            "b24a001a0e5c484580f24949ff700646"
          ]
        },
        "id": "E6Yff4klvXKC",
        "outputId": "421027bf-c5df-4bb3-946b-91bce3d49b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6b2ae9ca3b74b5797e114c97953ad88"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = '''If I have 5 apples and eat 2,\n",
        "                  but then find 4 more on my way home,\n",
        "                  how many do I have?'''"
      ],
      "metadata": {
        "id": "cqataWY31SJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_13b = pipeline_13b(input_text)[0]['generated_text']\n",
        "print(answer_13b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9pYcubsXTRo",
        "outputId": "59ee9598-299c-485f-e77b-f7aa8f21bd93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If I have 5 apples and eat 2,\n",
            "                  but then find 4 more on my way home,\n",
            "                  how many do I have?\n",
            "                  Answer: 7\n",
            "                </p>\n",
            "                <p>\n",
            "                  If I have 10 apples and eat 4,\n",
            "                  but then find 6 more on my way home,\n",
            "                  how many do I have?\n",
            "                  Answer: 10\n",
            "                </p>\n",
            "                <p>\n",
            "                  If I have 10 apples and eat 4,\n",
            "                  but then find 6 more on my way home,\n",
            "                  how many do I have?\n",
            "                  Answer: 10\n",
            "                </p>\n",
            "              </div>\n",
            "            </div>\n",
            "          </div>\n",
            "        </div>\n",
            "      </div>\n",
            "    </div>\n",
            "  </div>\n",
            ");\n",
            "\n",
            "export default App;\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_13b_chat = pipeline_13b_chat(input_text)[0]['generated_text']\n",
        "print(answer_13b_chat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggN1_KvHvfWe",
        "outputId": "bc01e96c-570d-495d-a8fd-8a625df47662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If I have 5 apples and eat 2,\n",
            "                  but then find 4 more on my way home,\n",
            "                  how many do I have?\n",
            "\n",
            "  Solution:\n",
            "  You have 6 apples.\n",
            "\n",
            "  Why:\n",
            "  When you started, you had 5 apples.\n",
            "  You ate 2, so you had 5 - 2 = 3 apples left.\n",
            "  Then, you found 4 more apples, so you have 3 + 4 = 7 apples.\n",
            "\n",
            "  So, the final answer is 7 apples.\n"
          ]
        }
      ]
    }
  ]
}
